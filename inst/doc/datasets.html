<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Datasets</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Datasets</h1>



<p>The following datasets can be used with the NeuroDecodeR to learn how
to use the package.</p>
<p><br></p>
<div id="zhang-desimone-7-object-dataset" class="section level1">
<h1>Zhang-Desimone 7 object dataset</h1>
<p>The Zhang-Desimone 7 object dataset was collected by Ying Zhang in <a href="http://mcgovern.mit.edu/principal-investigators/robert-desimone">Bob
Desimone’s lab</a> in the McGovern Institute at MIT. The data was used
in the supplemental figures in the paper <a href="https://pubmed.ncbi.nlm.nih.gov/21555594/">Object decoding with
attention in inferior temporal cortex, PNAS, 2011</a>.</p>
<p>The data consists of single unit recordings from the 132 neurons in
inferior temporal cortex (IT). The recordings were made while a monkey
viewed 7 different objects that were presented at three different
locations (the monkey was also shown images that consisted of three
objects shown simultaneously and had to perform an attention task,
however the dataset compiled here only consists of trials when single
objects were shown). Each object was presented approximately 20 times at
each of the three locations. The data is in <a href="data_formats.html">raster format</a>, and each trial consists of
500 ms of baseline data where a monkey viewed a fixation dot, and 500 ms
of data when a monkey viewed one of the 7 different images.</p>
<div id="accessing-the-dataset" class="section level3">
<h3>Accessing the dataset</h3>
<p>The Zhang-Desimone 7 object dataset comes with the NeuroDecodeR
package. Individual raster files can be found in the directory
<code>extdata/Zhang_Desimone_7object_raster_data_rda/</code>.
Additionally, binned data using 150 ms bins and 50 ms sampling intervals
can be found in the file <code>extdata/ZD_150bins_50sampled.Rda</code>.
The code below shows how to load a single raster data file, and the
binned data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load a raster format data file</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>raster_dir_name <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="fu">system.file</span>(<span class="st">&quot;extdata&quot;</span>, <span class="at">package =</span> <span class="st">&quot;NeuroDecodeR&quot;</span>), <span class="st">&quot;Zhang_Desimone_7object_raster_data_small_rda&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>file_name <span class="ot">&lt;-</span> <span class="st">&quot;bp1001spk_01A_raster_data.rda&quot;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">file.path</span>(raster_dir_name, file_name))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># load the 150 ms binned data</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>binned_file_name <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;extdata/ZD_150bins_50sampled.Rda&quot;</span>, <span class="at">package=</span><span class="st">&quot;NeuroDecodeR&quot;</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(binned_file_name)</span></code></pre></div>
<p><br></p>
<p><br></p>
</div>
</div>
<div id="qi-constantinidis-pre-and-post-training-dataset" class="section level1">
<h1>Qi-Constantinidis pre and post training dataset</h1>
<p>The Qi-Constantinidis pre and post training dataset was collected by
Xue-Lian Qi in <a href="https://lab.vanderbilt.edu/constantinidis-lab/home/">Christos
Constantinidis’ lab</a> in Department of Neurobiology and Anatomy, at
the Wake Forest University School of Medicine. The data consists of
single unit recordings from the prefrontal cortex (PFC) while monkeys
passively viewed images (in a feature and spatial configurations), and
also after monkey were trained to engaged in a delayed match-to-sample
task with these images.</p>
<p><strong>If you plan to use this data please cite the publications
below.</strong> More information about the experiments and data analyses
can be found in these papers as well.</p>
<p><em>Meyers E, Qi XL, Constantinidis C (2012). Incorporation of new
information into prefrontal cortical activity after learning working
memory tasks. Proceedings of the National Academy of Sciences,
109:4651-4656.</em></p>
<p><em>Qi X-L, Meyer T, Stanford TR, Constantinidis C (2011) Changes in
prefrontal neuronal activity after learning to perform a spatial working
memory task. Cereb Cortex 21:2722–2732</em></p>
<div id="accessing-the-dataset-1" class="section level3">
<h3>Accessing the dataset</h3>
<p>The MATLAB versions of the raster files can be downloaded from <a href="http://www.readout.info">www.readout.info</a> and can be converted
to R raster format data files using the
<code>convert_matlab_raster_data()</code> function.</p>
<p><br></p>
<p><br></p>
</div>
</div>
<div id="isik-26-letter-meg-dataset" class="section level1">
<h1>Isik 26 letter MEG dataset</h1>
<p>The Isik 26 letter MEG dataset was collected by Leyla Isik in Tommy
Poggio’s lab and the MEG Lab at the McGovern Institute at MIT. The data
was used in Figure 2b of the paper: <a href="https://journals.physiology.org/doi/full/10.1152/jn.00394.2013">The
dynamics of invariant object recognition in the human visual system,
J.</a> J. Neurophysiology, 2014.</p>
<p>The data consists of 306 channel (comprised of 102 magentometers, and
204 planar gradiometers) MEG recordings from an Elekta Neuromag Triux
Scanner. One subject was shown 26 black, upper-case letters, on a white
background, while their neural response was recorded in the MEG scanner.
Each letter was presented approximately 50 times. The data is in
raster-format, and each trial consists of 233 ms of baseline data where
the subject viewed a fixation cross, followed by 50 ms of data when the
subject viewed the image of one letter, and 417 ms of data when they
again viewed a fixation cross.</p>
<p>The data is available in two formats – the raw MEG files output by
the scanner (.fif format) and preprocessed data in <a href="data_formats.html">raster format</a>. The raw data download also
includes a file with raster labels indicating which stimulus was shown
in each trial.</p>
<div id="accessing-the-dataset-2" class="section level3">
<h3>Accessing the dataset</h3>
<p>The MATLAB versions of the raster files can be downloaded from <a href="http://www.readout.info">www.readout.info</a> and can be converted
to R raster format data files using the
<code>convert_matlab_raster_data()</code> function.</p>
<p><br></p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
